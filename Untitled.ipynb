{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7657c9ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inference.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2732552/3366967298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inference.engine'"
     ]
    }
   ],
   "source": [
    "from inference.engine import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6339ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inference\n",
      "  Obtaining dependency information for inference from https://files.pythonhosted.org/packages/e0/1f/1a7a07474fa4f833e336928240487bc494591c0413ca8fe855e8ffab7890/inference-0.7.2-1-py3-none-any.whl.metadata\n",
      "  Downloading inference-0.7.2-1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting APScheduler<=3.10.1 (from inference)\n",
      "  Downloading APScheduler-3.10.1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m902.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cython<=3.0.0 in ./anaconda3/lib/python3.9/site-packages (from inference) (0.29.24)\n",
      "Collecting fastapi<=0.85.1 (from inference)\n",
      "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m996.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.25.2 in ./anaconda3/lib/python3.9/site-packages (from inference) (1.22.4)\n",
      "Collecting opencv-python<=4.8.0.76 (from inference)\n",
      "  Obtaining dependency information for opencv-python<=4.8.0.76 from https://files.pythonhosted.org/packages/f5/d0/2e455d894ec0d6527e662ad55e70c04f421ad83a6fd0a54c3dd73c411282/opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting piexif<=1.1.3 (from inference)\n",
      "  Downloading piexif-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pillow<=9.5.0 in ./anaconda3/lib/python3.9/site-packages (from inference) (8.4.0)\n",
      "Collecting prometheus-fastapi-instrumentator<=6.0.0 (from inference)\n",
      "  Downloading prometheus_fastapi_instrumentator-6.0.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<=2.31.0 in ./anaconda3/lib/python3.9/site-packages (from inference) (2.26.0)\n",
      "Collecting rich<=13.5.2 (from inference)\n",
      "  Obtaining dependency information for rich<=13.5.2 from https://files.pythonhosted.org/packages/8d/5f/21a93b2ec205f4b79853ff6e838e3c99064d5dbe85ec6b05967506f14af0/rich-13.5.2-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shapely<=2.0.1 (from inference)\n",
      "  Downloading shapely-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting onnxruntime<=1.14.1 (from inference)\n",
      "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=0.7 in ./anaconda3/lib/python3.9/site-packages (from APScheduler<=3.10.1->inference) (58.0.4)\n",
      "Requirement already satisfied: six>=1.4.0 in ./anaconda3/lib/python3.9/site-packages (from APScheduler<=3.10.1->inference) (1.16.0)\n",
      "Requirement already satisfied: pytz in ./anaconda3/lib/python3.9/site-packages (from APScheduler<=3.10.1->inference) (2021.3)\n",
      "Collecting tzlocal!=3.*,>=2.0 (from APScheduler<=3.10.1->inference)\n",
      "  Obtaining dependency information for tzlocal!=3.*,>=2.0 from https://files.pythonhosted.org/packages/84/d2/730a87f0dbf184760394a85088d0d2366a5a8a32bc32ffd869a83f1de854/tzlocal-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 (from fastapi<=0.85.1->inference)\n",
      "  Obtaining dependency information for pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 from https://files.pythonhosted.org/packages/02/bd/2c6a34eeffcc437fca7d5c4b6cf7745fcc806842de5fced482d4cdba55f0/pydantic-1.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting starlette==0.20.4 (from fastapi<=0.85.1->inference)\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in ./anaconda3/lib/python3.9/site-packages (from starlette==0.20.4->fastapi<=0.85.1->inference) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in ./anaconda3/lib/python3.9/site-packages (from starlette==0.20.4->fastapi<=0.85.1->inference) (4.7.1)\n",
      "Collecting coloredlogs (from onnxruntime<=1.14.1->inference)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers in ./anaconda3/lib/python3.9/site-packages (from onnxruntime<=1.14.1->inference) (23.5.26)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.9/site-packages (from onnxruntime<=1.14.1->inference) (21.0)\n",
      "Requirement already satisfied: protobuf in ./anaconda3/lib/python3.9/site-packages (from onnxruntime<=1.14.1->inference) (3.20.3)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.9/site-packages (from onnxruntime<=1.14.1->inference) (1.9)\n",
      "Requirement already satisfied: prometheus-client<1.0.0,>=0.8.0 in ./anaconda3/lib/python3.9/site-packages (from prometheus-fastapi-instrumentator<=6.0.0->inference) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests<=2.31.0->inference) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests<=2.31.0->inference) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests<=2.31.0->inference) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests<=2.31.0->inference) (3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./anaconda3/lib/python3.9/site-packages (from rich<=13.5.2->inference) (2.2.0)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich<=13.5.2->inference)\n",
      "  Obtaining dependency information for pygments<3.0.0,>=2.13.0 from https://files.pythonhosted.org/packages/43/88/29adf0b44ba6ac85045e63734ae0997d3c58d8b1a91c914d240828d0d73d/Pygments-2.16.1-py3-none-any.whl.metadata\n",
      "  Downloading Pygments-2.16.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<=13.5.2->inference) (0.1.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<=1.14.1->inference)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging->onnxruntime<=1.14.1->inference) (3.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.9/site-packages (from sympy->onnxruntime<=1.14.1->inference) (1.2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi<=0.85.1->inference) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup in ./anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi<=0.85.1->inference) (1.1.3)\n",
      "Downloading inference-0.7.2-1-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading rich-13.5.2-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Pygments-2.16.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.0.1-py3-none-any.whl (20 kB)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tzlocal, shapely, pygments, pydantic, piexif, opencv-python, humanfriendly, starlette, rich, coloredlogs, APScheduler, onnxruntime, fastapi, prometheus-fastapi-instrumentator, inference\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.10.0\n",
      "    Uninstalling Pygments-2.10.0:\n",
      "      Successfully uninstalled Pygments-2.10.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.2.0\n",
      "    Uninstalling pydantic-2.2.0:\n",
      "      Successfully uninstalled pydantic-2.2.0\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.27.0\n",
      "    Uninstalling starlette-0.27.0:\n",
      "      Successfully uninstalled starlette-0.27.0\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.101.1\n",
      "    Uninstalling fastapi-0.101.1:\n",
      "      Successfully uninstalled fastapi-0.101.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed APScheduler-3.10.1 coloredlogs-15.0.1 fastapi-0.85.1 humanfriendly-10.0 inference-0.7.2 onnxruntime-1.14.1 opencv-python-4.8.0.76 piexif-1.1.3 prometheus-fastapi-instrumentator-6.0.0 pydantic-1.10.12 pygments-2.16.1 rich-13.5.2 shapely-2.0.1 starlette-0.20.4 tzlocal-5.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a37a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] += \":/content/fairseq/\"\n",
    "# sanity check to see if fairseq is installed\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050e55b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883eccf74bae499cad08640a88a00e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b3156b6b2745cfa18d2f88c48fa8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 20:35:35 | INFO | torch.distributed.nn.jit.instantiator | Created a temporary directory at /tmp/tmpcw1578qa\n",
      "2023-08-18 20:35:35 | INFO | torch.distributed.nn.jit.instantiator | Writing /tmp/tmpcw1578qa/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70cff24a1154d1baa7570c253fa12d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['sop_classifier.classifier.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'sop_classifier.classifier.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
    "model = AutoModel.from_pretrained('ai4bharat/indic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3498a31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'AlbertModel' is not supported for translation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"ory_Orya\", tgt_lang='eng_Latn', max_length = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a16bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-18 23:42:21--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
      "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 101.53.136.18, 101.53.152.30, 164.52.210.58, ...\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.136.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2023-08-18 23:42:22 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da666a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-18 23:43:43--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 101.53.136.19, 164.52.210.96, 164.52.210.58, ...\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.136.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5236582400 (4.9G) [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar             100%[===================>]   4.88G  3.23MB/s    in 33m 38s \n",
      "\n",
      "2023-08-19 00:17:21 (2.47 MB/s) - ‘m2m.tar’ saved [5236582400/5236582400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "194910e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  m2m.tar\n",
      "error: End-of-centdir-64 signature not where expected (prepended bytes?)\n",
      "  (attempting to process anyway)\n",
      "warning [m2m.tar]:  5346816 extra bytes at beginning or within zipfile\n",
      "  (attempting to process anyway)\n",
      "replace archive/data.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip m2m.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8def3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'testing'\n",
      "/home/f20212416/testing/indicTrans\n",
      "[Errno 2] No such file or directory: 'indicTrans'\n",
      "/home/f20212416/testing/indicTrans\n",
      "/home/f20212416/testing\n"
     ]
    }
   ],
   "source": [
    "%cd testing\n",
    "%cd indicTrans\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61d8dfb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'subword_nmt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2732552/3594967752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindicTrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/testing/indicTrans/inference/engine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msacremoses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMosesTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msacremoses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMosesDetokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msubword_nmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_bpe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'subword_nmt'"
     ]
    }
   ],
   "source": [
    "from indicTrans.inference.engine import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71e196c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'indicTrans'\n",
      "/home/f20212416/testing/indicTrans\n"
     ]
    }
   ],
   "source": [
    "%cd indicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b192374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f20212416/testing\n"
     ]
    }
   ],
   "source": [
    "%cd testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f09b56f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f20212416\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b443cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indicTrans'...\n",
      "remote: Enumerating objects: 697, done.\u001b[K\n",
      "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
      "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
      "remote: Total 697 (delta 278), reused 344 (delta 240), pack-reused 297\u001b[K\n",
      "Receiving objects: 100% (697/697), 2.64 MiB | 4.74 MiB/s, done.\n",
      "Resolving deltas: 100% (405/405), done.\n",
      "/home/f20212416/indicTrans\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
    "%cd indicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d83742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indic_nlp_library'...\n",
      "remote: Enumerating objects: 1396, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
      "remote: Total 1396 (delta 133), reused 119 (delta 105), pack-reused 1219\u001b[K\n",
      "Receiving objects: 100% (1396/1396), 9.57 MiB | 7.89 MiB/s, done.\n",
      "Resolving deltas: 100% (743/743), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "944a51fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indic_nlp_resources'...\n",
      "remote: Enumerating objects: 139, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
      "Receiving objects: 100% (139/139), 149.77 MiB | 9.36 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6353ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'subword-nmt'...\n",
      "remote: Enumerating objects: 597, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
      "Receiving objects: 100% (597/597), 252.23 KiB | 727.00 KiB/s, done.\n",
      "Resolving deltas: 100% (357/357), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rsennrich/subword-nmt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f037eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f20212416\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea79d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in ./anaconda3/lib/python3.9/site-packages (0.0.53)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: mock in ./anaconda3/lib/python3.9/site-packages (4.0.3)\n",
      "Requirement already satisfied: sacrebleu in ./anaconda3/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: tensorboardX in ./anaconda3/lib/python3.9/site-packages (2.6.2)\n",
      "Requirement already satisfied: pyarrow in ./anaconda3/lib/python3.9/site-packages (12.0.1)\n",
      "Requirement already satisfied: indic-nlp-library in ./anaconda3/lib/python3.9/site-packages (0.92)\n",
      "Requirement already satisfied: regex in ./anaconda3/lib/python3.9/site-packages (from sacremoses) (2021.8.3)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.9/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.9/site-packages (from sacremoses) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.9/site-packages (from sacremoses) (1.1.0)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.9/site-packages (from sacremoses) (4.62.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./anaconda3/lib/python3.9/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: portalocker in ./anaconda3/lib/python3.9/site-packages (from sacrebleu) (2.7.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./anaconda3/lib/python3.9/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./anaconda3/lib/python3.9/site-packages (from sacrebleu) (0.4.4)\n",
      "Requirement already satisfied: lxml in ./anaconda3/lib/python3.9/site-packages (from sacrebleu) (4.6.3)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.9/site-packages (from tensorboardX) (21.0)\n",
      "Requirement already satisfied: protobuf in ./anaconda3/lib/python3.9/site-packages (from tensorboardX) (3.20.3)\n",
      "Requirement already satisfied: sphinx-argparse in ./anaconda3/lib/python3.9/site-packages (from indic-nlp-library) (0.4.0)\n",
      "Requirement already satisfied: sphinx-rtd-theme in ./anaconda3/lib/python3.9/site-packages (from indic-nlp-library) (1.2.2)\n",
      "Requirement already satisfied: morfessor in ./anaconda3/lib/python3.9/site-packages (from indic-nlp-library) (2.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging->tensorboardX) (3.0.4)\n",
      "Requirement already satisfied: sphinx>=1.2.0 in ./anaconda3/lib/python3.9/site-packages (from sphinx-argparse->indic-nlp-library) (4.2.0)\n",
      "Requirement already satisfied: docutils<0.19 in ./anaconda3/lib/python3.9/site-packages (from sphinx-rtd-theme->indic-nlp-library) (0.17.1)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in ./anaconda3/lib/python3.9/site-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.3 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
      "Requirement already satisfied: Pygments>=2.0 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
      "Requirement already satisfied: babel>=1.3 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.9.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
      "Requirement already satisfied: imagesize in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.5.0 in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.26.0)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (58.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.9/site-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.2)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d6e58f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mosestokenizer in ./anaconda3/lib/python3.9/site-packages (1.2.1)\n",
      "Collecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: docopt in ./anaconda3/lib/python3.9/site-packages (from mosestokenizer) (0.6.2)\n",
      "Requirement already satisfied: openfile in ./anaconda3/lib/python3.9/site-packages (from mosestokenizer) (0.0.7)\n",
      "Requirement already satisfied: uctools in ./anaconda3/lib/python3.9/site-packages (from mosestokenizer) (1.3.0)\n",
      "Requirement already satisfied: toolwrapper in ./anaconda3/lib/python3.9/site-packages (from mosestokenizer) (2.1.0)\n",
      "Requirement already satisfied: mock in ./anaconda3/lib/python3.9/site-packages (from subword-nmt) (4.0.3)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.9/site-packages (from subword-nmt) (4.62.3)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: subword-nmt\n",
      "Successfully installed subword-nmt-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install mosestokenizer subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f591b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 34777, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 34777 (delta 0), reused 1 (delta 0), pack-reused 34769\u001b[K\n",
      "Receiving objects: 100% (34777/34777), 25.02 MiB | 9.37 MiB/s, done.\n",
      "Resolving deltas: 100% (25221/25221), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/fairseq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b461f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f20212416/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbf7b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/f20212416/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cffi in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (1.14.6)\n",
      "Requirement already satisfied: cython in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (0.29.24)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (1.0.7)\n",
      "Requirement already satisfied: omegaconf<2.1 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.21.3 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (1.22.4)\n",
      "Requirement already satisfied: regex in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (2021.8.3)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (2.3.1)\n",
      "Requirement already satisfied: torch>=1.13 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (4.62.3)\n",
      "Requirement already satisfied: bitarray in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (2.3.0)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (0.24.2)\n",
      "Requirement already satisfied: packaging in /home/f20212416/anaconda3/lib/python3.9/site-packages (from fairseq==0.12.2) (21.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/f20212416/anaconda3/lib/python3.9/site-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/f20212416/anaconda3/lib/python3.9/site-packages (from omegaconf<2.1->fairseq==0.12.2) (4.7.1)\n",
      "Requirement already satisfied: portalocker in /home/f20212416/anaconda3/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.7.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/f20212416/anaconda3/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.4)\n",
      "Requirement already satisfied: lxml in /home/f20212416/anaconda3/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.6.3)\n",
      "Requirement already satisfied: filelock in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (3.3.1)\n",
      "Requirement already satisfied: sympy in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (1.9)\n",
      "Requirement already satisfied: networkx in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (2.11.3)\n",
      "Requirement already satisfied: pycparser in /home/f20212416/anaconda3/lib/python3.9/site-packages (from cffi->fairseq==0.12.2) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from packaging->fairseq==0.12.2) (3.0.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from scikit-learn->fairseq==0.12.2) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from scikit-learn->fairseq==0.12.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from scikit-learn->fairseq==0.12.2) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.2.1)\n",
      "Building wheels for collected packages: fairseq\n",
      "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp39-cp39-linux_x86_64.whl size=11678409 sha256=9c266f0443fd1e207c2f1406c7d2d23e35f9c47ec0036b91a431220d718ae227\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lfuqqke3/wheels/90/a3/a8/15bec5168b11865950c47e9a386953c77a6d225f964e960133\n",
      "Successfully built fairseq\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 0.12.2\n",
      "    Uninstalling fairseq-0.12.2:\n",
      "      Successfully uninstalled fairseq-0.12.2\n",
      "Successfully installed fairseq-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "477c625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xformers in /home/f20212416/anaconda3/lib/python3.9/site-packages (0.0.20)\n",
      "Requirement already satisfied: numpy in /home/f20212416/anaconda3/lib/python3.9/site-packages (from xformers) (1.22.4)\n",
      "Requirement already satisfied: pyre-extensions==0.0.29 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from xformers) (0.0.29)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from xformers) (2.0.1)\n",
      "Requirement already satisfied: typing-inspect in /home/f20212416/anaconda3/lib/python3.9/site-packages (from pyre-extensions==0.0.29->xformers) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/f20212416/anaconda3/lib/python3.9/site-packages (from pyre-extensions==0.0.29->xformers) (4.7.1)\n",
      "Requirement already satisfied: filelock in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->xformers) (3.3.1)\n",
      "Requirement already satisfied: sympy in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->xformers) (1.9)\n",
      "Requirement already satisfied: networkx in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->xformers) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->xformers) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.0.1->xformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.1->xformers) (1.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/f20212416/anaconda3/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers) (0.4.3)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m/home/f20212416\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68c7a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] += \":/content/fairseq/\"\n",
    "# sanity check to see if fairseq is installed\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5cfa39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f20212416/indicTrans\n"
     ]
    }
   ],
   "source": [
    "%cd indicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8dfb9ec9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inference.custom_interactive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2732552/3594967752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindicTrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/testing/indicTrans/inference/engine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mindicnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_interactive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inference.custom_interactive'"
     ]
    }
   ],
   "source": [
    "from indicTrans.inference.engine import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6057919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f20212416\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e329b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1518465\n",
      "Healthcare sentences: 74413\n",
      "Governance sentences: 226600\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def count_classifications(csv_file):\n",
    "    health_count = 0\n",
    "    gov_count = 0\n",
    "    total_count = 0\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            total_count += 1\n",
    "            if row['Classification'] == 'healthcare':\n",
    "                health_count += 1\n",
    "            elif row['Classification'] == 'governance':\n",
    "                gov_count += 1\n",
    "\n",
    "    return health_count, gov_count, total_count\n",
    "\n",
    "# Provide the path for the generated CSV file\n",
    "csv_file_path = 'classification_odia.csv'\n",
    "\n",
    "healthcare_count, governance_count, total_count = count_classifications(csv_file_path)\n",
    "print(f\"Total sentences: {total_count}\")\n",
    "print(f\"Healthcare sentences: {healthcare_count}\")\n",
    "print(f\"Governance sentences: {governance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1049b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  6 18:31:27 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:2B:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    73W / 250W |   3341MiB / 40960MiB |     33%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2880      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   3591902      C   ...2416/anaconda3/bin/python     3334MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d304812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = \"classification_hindi.csv\"\n",
    "governance_output_path = \"governance_hindi_2.csv\"\n",
    "healthcare_output_path = \"healthcare_hindi_2.csv\"\n",
    "\n",
    "# Initialize lists to store data\n",
    "governance_data = []\n",
    "healthcare_data = []\n",
    "\n",
    "# Read data from the input file\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    for row in reader:\n",
    "        hindi_sentence = row['Hindi']\n",
    "        english_sentence = row['English']\n",
    "        classification = row['Classification']\n",
    "        \n",
    "        if classification == 'governance':\n",
    "            governance_data.append({'hindi': hindi_sentence, 'english': english_sentence, 'classification': classification})\n",
    "        elif classification == 'healthcare':\n",
    "            healthcare_data.append({'hindi': hindi_sentence, 'english': english_sentence, 'classification': classification})\n",
    "\n",
    "# Write 'governance' data to a CSV file\n",
    "with open(governance_output_path, 'a', newline='', encoding='utf-8') as governance_output_file:\n",
    "    writer = csv.DictWriter(governance_output_file, fieldnames=['hindi', 'english', 'classification'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(governance_data)\n",
    "\n",
    "# Write 'healthcare' data to a CSV file\n",
    "with open(healthcare_output_path, 'a', newline='', encoding='utf-8') as healthcare_output_file:\n",
    "    writer = csv.DictWriter(healthcare_output_file, fieldnames=['hindi', 'english', 'classification'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(healthcare_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92e4c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with classification 'healthcare' copied to odia_to_hindi_governance.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = \"hindi_to_odia_governance.csv\"\n",
    "output_file_path = \"odia_to_hindi_governance.csv\"\n",
    "\n",
    "# Open the input file in read mode and the output file in append mode\n",
    "with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file, \\\n",
    "     open(output_file_path, 'a', newline='', encoding='utf-8') as output_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    writer = csv.DictWriter(output_file, fieldnames=reader.fieldnames)\n",
    "    \n",
    "    # Write header to the output file if it's a new file\n",
    "#     if output_file.tell() == 0:\n",
    "#         writer.writeheader()\n",
    "    \n",
    "    # Copy rows with classification \"healthcare\" to the output file\n",
    "    for row in reader:\n",
    "        if row['classification'] == 'governance':\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"Rows with classification 'healthcare' copied to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8830b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed in odia_to_hindi_governance.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file_path = \"odia_to_hindi_governance.csv\"\n",
    "\n",
    "# Read the data from the input file\n",
    "data = pd.read_csv(input_file_path)\n",
    "\n",
    "# Remove duplicates based on 'odia' and 'hindi' columns\n",
    "data_no_duplicates = data.drop_duplicates(subset=['odia', 'hindi'], keep='first')\n",
    "\n",
    "# Save the data with duplicates removed back to the input file\n",
    "data_no_duplicates.to_csv(input_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Duplicates removed in {input_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56de4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with classification 'governance' copied to odia_to_hindi_governance.csv.\n",
      "Rows with classification 'healthcare' copied to odia_to_hindi_healthcare.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = \"hindi2odia.csv\"\n",
    "output_governance_path = \"odia_to_hindi_governance.csv\"\n",
    "output_healthcare_path = \"odia_to_hindi_healthcare.csv\"\n",
    "\n",
    "# Create dictionaries to map the column names between the input and output files\n",
    "column_mapping_governance = {'hindi': 'hindi', 'odia': 'odia', 'classification': 'classification'}\n",
    "column_mapping_healthcare = {'hindi': 'hindi', 'odia': 'odia', 'classification': 'classification'}\n",
    "\n",
    "# Open the input file in read mode\n",
    "with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    \n",
    "    # Check the classification column and copy to the appropriate output file\n",
    "    for row in reader:\n",
    "        classification = row['classification']\n",
    "        if classification == 'governance':\n",
    "            # Copy to odia_to_hindi_governance.csv with the appropriate column mapping\n",
    "            with open(output_governance_path, 'a', newline='', encoding='utf-8') as output_governance_file:\n",
    "                writer = csv.DictWriter(output_governance_file, fieldnames=column_mapping_governance.values())\n",
    "                \n",
    "                # Write header if the file is empty\n",
    "                if output_governance_file.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row with the mapped columns\n",
    "                writer.writerow({column_mapping_governance[key]: row[key] for key in row})\n",
    "        elif classification == 'healthcare':\n",
    "            # Copy to odia_to_hindi_healthcare.csv with the appropriate column mapping\n",
    "            with open(output_healthcare_path, 'a', newline='', encoding='utf-8') as output_healthcare_file:\n",
    "                writer = csv.DictWriter(output_healthcare_file, fieldnames=column_mapping_healthcare.values())\n",
    "                \n",
    "                # Write header if the file is empty\n",
    "                if output_healthcare_file.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row with the mapped columns\n",
    "                writer.writerow({column_mapping_healthcare[key]: row[key] for key in row})\n",
    "\n",
    "print(f\"Rows with classification 'governance' copied to {output_governance_path}.\")\n",
    "print(f\"Rows with classification 'healthcare' copied to {output_healthcare_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aac438ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data corrected and saved to odia_to_hindi_healthcare.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = \"odia_to_hindi_healthcare_incorrect.csv\"\n",
    "output_file_path = \"odia_to_hindi_healthcare.csv\"\n",
    "\n",
    "# Define the line number where the mismatch occurred\n",
    "mismatched_line_number = 96956  # Adjust to the actual line number\n",
    "\n",
    "# Open the input file in read mode and the output file in write mode\n",
    "with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file, \\\n",
    "     open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    fieldnames = reader.fieldnames\n",
    "\n",
    "    # Write the header to the output file\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Loop through the rows and correct the mismatch\n",
    "    for line_number, row in enumerate(reader, start=1):\n",
    "        if line_number >= mismatched_line_number:\n",
    "            # Swap the content of 'odia' and 'hindi' columns\n",
    "            row['odia'], row['hindi'] = row['hindi'], row['odia']\n",
    "\n",
    "        # Write the corrected row to the output file\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Data corrected and saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c30fbdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1435691\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_path = 'classification_odia_full.csv'\n",
    "with open(csv_file_path,'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670c65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
