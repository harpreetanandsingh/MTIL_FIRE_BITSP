{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351a0666",
   "metadata": {},
   "source": [
    "## Inference for odia-hindi healthcare model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f874bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 11:38:21.304616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-08 11:38:21.366956: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-08 11:38:22.651563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'शैले में आप स्वास्थ्य मापने की यंत्र है ।'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"harupurito/or_hin_healthcare\")\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"ory_Orya\", tgt_lang='hin_Deva', max_length=400)\n",
    "\n",
    "odia_sentence = \"ଶାଳାରେ ଆପଣ ସ୍ୱାସ୍ଥ୍ୟ ମାପଣର ଯନ୍ତ୍ର ରହିଛି।\"\n",
    "hindi_translation = translator(odia_sentence)\n",
    "print(hindi_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3f16b",
   "metadata": {},
   "source": [
    "## Inference for hindi-odia healthcare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46da2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578acba1010c4ece9774c953b006cf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c2467781ed4531b034c04c5fd830f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ଶୈଳରେ ଆପଣ ସ୍ୱାସ୍ଥ୍ୟ ମାପିବା ଯନ୍ତ୍ରାଂଶ ଅଛନ୍ତି।'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"harupurito/hin_or_healthcare\")\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"hin_Deva\", tgt_lang='ory_Orya', max_length=400)\n",
    "\n",
    "odia_sentence = \"शैले में आप स्वास्थ्य मापने की यंत्र है ।\"\n",
    "hindi_translation = translator(odia_sentence)\n",
    "print(hindi_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ab65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97f53d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef754d44d8aa4197a73fe9b6c8eb74e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/896 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d543c285af846959ba434ae48446f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1e852021d34cab8fe1706fb3392752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'source_hi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3691551/2078816348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0modia_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mhindi_translation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modia_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'translation_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'source_hi'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0modia_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target_odi'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhindi_translation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source_hi'"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
    "import csv\n",
    "input_csv_path = \"Task2_hi_odi_GOV_1000.tsv\"\n",
    "output_csv_path = \"Task2_hi_odi_GOV_1000.csv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"harupurito/hin_or_governance\")\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"hin_Deva\", tgt_lang='ory_Orya', max_length=400)\n",
    "\n",
    "# Write translated data to the output CSV file one by one\n",
    "header = ['source_hi', 'target_odi']\n",
    "\n",
    "with open(input_csv_path, 'r', encoding='utf-8') as input_file, open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    writer = csv.DictWriter(output_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in reader:\n",
    "        odia_sentence = row['source_hi']\n",
    "        hindi_translation = translator(odia_sentence)[0]['translation_text']\n",
    "        writer.writerow({'source_hi': odia_sentence, 'target_odi': hindi_translation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "194fa29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import csv\n",
    "\n",
    "input_tsv_path = \"Task1_hi_odi_2000.tsv\"  # Update the TSV file path\n",
    "output_csv_path = \"Task1_hi_odi_2000_nllb.csv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"hin_Deva\", tgt_lang='ory_Orya', max_length=400)\n",
    "\n",
    "# Write translated data to the output CSV file one by one\n",
    "header = ['source_hi', 'target_odi']\n",
    "\n",
    "with open(input_tsv_path, 'r', encoding='utf-8') as input_file, open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    tsv_reader = csv.reader(input_file, delimiter='\\t')\n",
    "    writer = csv.DictWriter(output_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in tsv_reader:\n",
    "        if len(row) >= 1:\n",
    "            odia_sentence = row[0]\n",
    "            hindi_translation = translator(odia_sentence)[0]['translation_text']\n",
    "            writer.writerow({'source_hi': odia_sentence, 'target_odi': hindi_translation})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b02fe450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 429 is bigger than 0.9 * max_length: 400. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import csv\n",
    "\n",
    "input_tsv_path = \"Task1_odi_hi_2000.tsv\"  # Update the TSV file path\n",
    "output_csv_path = \"Task1_odi_hi_HLT_nllb.csv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=\"ory_Orya\", tgt_lang='hin_Deva', max_length=400)\n",
    "\n",
    "# Write translated data to the output CSV file one by one\n",
    "header = ['source_odi', 'target_hi']\n",
    "\n",
    "with open(input_tsv_path, 'r', encoding='utf-8') as input_file, open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    tsv_reader = csv.reader(input_file, delimiter='\\t')\n",
    "    writer = csv.DictWriter(output_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in tsv_reader:\n",
    "        if len(row) >= 1:\n",
    "            odia_sentence = row[0]\n",
    "            hindi_translation = translator(odia_sentence)[0]['translation_text']\n",
    "            writer.writerow({'source_odi': odia_sentence, 'target_hi': hindi_translation})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07ebf40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun Sep 10 00:24:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:2B:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    58W / 250W |  40335MiB / 40960MiB |     25%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2880      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    844598      C   python                           5012MiB |\n",
      "|    0   N/A  N/A    889331      C   ...2416/anaconda3/bin/python      824MiB |\n",
      "|    0   N/A  N/A   3659900      C   ...2416/anaconda3/bin/python    29794MiB |\n",
      "|    0   N/A  N/A   3663958      C   ...2416/anaconda3/bin/python     4698MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc117ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
