{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62776231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/f20212416/.cache/torch/sentence_transformers/google_muril-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/f20212416/.cache/torch/sentence_transformers/google_muril-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9901, device='cuda:0') tensor(0.9899, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9899138808250427"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "model = SentenceTransformer(\"google/muril-base-cased\")\n",
    "\n",
    "sentence = \"कुंवारी कन्या में यह केवल पिन की नोक जैसा होता है|\"\n",
    "sentence_1 = \"କୁମାରୀ କନ୍ୟାମାନଙ୍କରେ ଏହା କେବଳ ପିନର ଧାର ଭଳି ହୋଇଥାଏ।\"\n",
    "sentence_2 = \"କୁମାରୀ କନ୍ୟାଙ୍କଠାରେ ଏହା କେବଳ ପିନର ଚଟାଣ ପରି ହୋଇଥାଏ\"\n",
    "#Compute embedding for both lists\n",
    "embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "embeddings1 = model.encode(sentence_1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentence_2, convert_to_tensor=True)\n",
    "\n",
    "cosine_score_1 = util.cos_sim(embedding, embeddings1)\n",
    "cosine_score_2 = util.cos_sim(embedding, embeddings2)\n",
    "\n",
    "a = cosine_score_1.item()\n",
    "a = cosine_score_1.item()\n",
    "#sentences1[0]\n",
    "#Output the pairs with their score\n",
    "# for i in range(len(sentences1)):\n",
    "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c813df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (4.28.1)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (0.24.2)\n",
      "Requirement already satisfied: scipy in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (1.7.1)\n",
      "Requirement already satisfied: nltk in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (3.6.5)\n",
      "Requirement already satisfied: sentencepiece in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./anaconda3/lib/python3.9/site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.0)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (1.9)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (2.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.8.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.9/site-packages (from nltk->sentence_transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./anaconda3/lib/python3.9/site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.2.1)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=ed23652f21784ee7af12cf2a1abddca6a72bfa631b56e648807b1c100eb5e616\n",
      "  Stored in directory: /home/f20212416/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence_transformers\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6efa727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 10 00:25:42 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:2B:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    76W / 250W |   5843MiB / 40960MiB |     17%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2880      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    844598      C   python                           5012MiB |\n",
      "|    0   N/A  N/A    889331      C   ...2416/anaconda3/bin/python      824MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "input_csv_file_nllb = 'Task1_hi_odi_2000_nllb.csv'\n",
    "input_csv_file_IT = 'Task1_hi_odi_2000_IT.csv'\n",
    "input_tsv_file = 'Task1_hi_odi_2000.tsv'\n",
    "\n",
    "output_csv_file = 'Task1_hi_odi_compared.csv'\n",
    "#with open(input_tsv_file, 'r', encoding='utf-8') as input_tsv_file, open(input_csv_file_nllb, 'r', newline='', encoding='utf-8') as input_file_nllb, open(input_csv_file_IT, 'r', newline='', encoding='utf-8'), open(output_csv_file, 'w', newline='', encoding='utf-8'):\n",
    "nllb_data = []\n",
    "it_data = []\n",
    "input_data = []\n",
    "with open(input_csv_file_nllb, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        odia_sentence = row['target_odi']\n",
    "        nllb_data.append({'target_odi': odia_sentence})\n",
    "with open(input_csv_file_nllb, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        odia_sentence = row['target_odi']\n",
    "        it_data.append({'target_odi': odia_sentence})\n",
    "with open(input_tsv_file, 'r') as file:\n",
    "    reader = csv.DictReader(file, delimiter ='\\t')\n",
    "    for row in reader:\n",
    "        hindi_sentence = row['source_hi']\n",
    "        input_data.append({'source_hi': hindi_sentence})\n",
    "\n",
    "with open(output_csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['source_hi', 'nllb', 'nllb_score', 'it', 'it_score'])  # Write header\n",
    "    \n",
    "    for :\n",
    "        odia_sentence = data['source_odi']\n",
    "        hindi_translation = indic2indic_model.batch_translate([odia_sentence], 'or', 'hi')\n",
    "        hindi_sentence = hindi_translation[0]\n",
    "        #classification = data['classification']\n",
    "writer.writerow([odia_sentence, hindi_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37bcbe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/f20212416/.cache/torch/sentence_transformers/google_muril-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/f20212416/.cache/torch/sentence_transformers/google_muril-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer(\"google/muril-base-cased\")\n",
    "\n",
    "# Define the paths to your TSV file and output CSV file\n",
    "input_tsv_path = \"Task1_hi_odi_2000.tsv\"  # Update with your TSV file path\n",
    "output_csv_path = \"Task1_hi_odi_compared.csv\"\n",
    "nllb_csv_path = \"Task1_hi_odi_2000_nllb.csv\"  # Path to the NLLB translations CSV file\n",
    "indictrans_csv_path = \"Task1_hi_odi_2000_IT.csv\"  # Path to the IndicTrans translations CSV file\n",
    "\n",
    "# Initialize lists to store the data\n",
    "hindi_sentences = []\n",
    "translations_nllb = []\n",
    "cosine_scores_nllb = []\n",
    "translations_indictrans = []\n",
    "cosine_scores_indictrans = []\n",
    "\n",
    "# Read sentences from the TSV file and calculate cosine similarities\n",
    "with open(input_tsv_path, 'r', encoding='utf-8') as tsv_file:\n",
    "    tsv_reader = csv.DictReader(tsv_file, delimiter='\\t')\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        if len(row) >= 1:\n",
    "            hindi_sentence = row['source_hi']\n",
    "\n",
    "            # Append the Hindi sentence\n",
    "            hindi_sentences.append(hindi_sentence)\n",
    "\n",
    "# Read NLLB translations from the CSV file\n",
    "with open(nllb_csv_path, 'r', encoding='utf-8') as nllb_file:\n",
    "    nllb_reader = csv.DictReader(nllb_file)\n",
    "    \n",
    "    for row in nllb_reader:\n",
    "        if len(row) >= 1:\n",
    "#             translation_nllb = row[0]\n",
    "            hindi_sentence = row['source_hi']\n",
    "            translation_nllb = row['target_odi']\n",
    "            # Compute embeddings for the sentences\n",
    "            embedding = model.encode(hindi_sentence, convert_to_tensor=True)\n",
    "            embeddings_nllb = model.encode(translation_nllb, convert_to_tensor=True)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_score_nllb = util.cos_sim(embedding, embeddings_nllb).item()\n",
    "\n",
    "            # Append the NLLB translation and cosine score\n",
    "            translations_nllb.append(translation_nllb)\n",
    "            cosine_scores_nllb.append(cosine_score_nllb)\n",
    "\n",
    "# Read IndicTrans translations from the CSV file\n",
    "with open(indictrans_csv_path, 'r', encoding='utf-8') as indictrans_file:\n",
    "    indictrans_reader = csv.DictReader(indictrans_file)\n",
    "    \n",
    "    for row in indictrans_reader:\n",
    "        if len(row) >= 1:\n",
    "            #translation_indictrans = row[0]\n",
    "            hindi_sentence = row['source_hi']\n",
    "            translation_indictrans = row['target_odi']\n",
    "            # Compute embeddings for the sentences\n",
    "            embedding = model.encode(hindi_sentence, convert_to_tensor=True)\n",
    "            embeddings_indictrans = model.encode(translation_indictrans, convert_to_tensor=True)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_score_indictrans = util.cos_sim(embedding, embeddings_indictrans).item()\n",
    "\n",
    "            # Append the IndicTrans translation and cosine score\n",
    "            translations_indictrans.append(translation_indictrans)\n",
    "            cosine_scores_indictrans.append(cosine_score_indictrans)\n",
    "\n",
    "# Create a CSV file with the desired columns\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['hindi_sentence', 'translation_nllb', 'cosine_nllb', 'translation_indictrans', 'cosine_indictrans'])\n",
    "\n",
    "    for i in range(len(hindi_sentences)):\n",
    "        writer.writerow([hindi_sentences[i], translations_nllb[i], cosine_scores_nllb[i], translations_indictrans[i], cosine_scores_indictrans[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d3ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/f20212416/.cache/torch/sentence_transformers/google_muril-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/f20212416/.cache/torch/sentence_transformers/google_muril-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer(\"google/muril-base-cased\")\n",
    "\n",
    "# Define the paths to your TSV file and output CSV file\n",
    "input_tsv_path = \"Task1_odi_hi_2000.tsv\"  # Update with your TSV file path\n",
    "output_csv_path = \"Task1_odi_hi_higher.csv\"\n",
    "nllb_csv_path = \"Task1_odi_hi_2000_nllb.csv\"  # Path to the NLLB translations CSV file\n",
    "indictrans_csv_path = \"Task1_odi_hi_2000_IT.csv\"  # Path to the IndicTrans translations CSV file\n",
    "\n",
    "# Initialize lists to store the data\n",
    "hindi_sentences = []\n",
    "selected_translations = []\n",
    "\n",
    "# Read sentences from the TSV file and calculate cosine similarities\n",
    "with open(input_tsv_path, 'r', encoding='utf-8') as tsv_file:\n",
    "    tsv_reader = csv.DictReader(tsv_file, delimiter='\\t')\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        if len(row) >= 1:\n",
    "            hindi_sentence = row['source_odi']\n",
    "            hindi_sentences.append(hindi_sentence)\n",
    "\n",
    "# Read NLLB and IndicTrans translations and calculate cosine similarities\n",
    "with open(nllb_csv_path, 'r', encoding='utf-8') as nllb_file, open(indictrans_csv_path, 'r', encoding='utf-8') as indictrans_file:\n",
    "    nllb_reader = csv.DictReader(nllb_file)\n",
    "    indictrans_reader = csv.DictReader(indictrans_file)\n",
    "    \n",
    "    for hindi_sentence, nllb_row, indictrans_row in zip(hindi_sentences, nllb_reader, indictrans_reader):\n",
    "        if len(nllb_row) >= 1 and len(indictrans_row) >= 1:\n",
    "            translation_nllb = nllb_row['target_hi']\n",
    "            translation_indictrans = indictrans_row['target_hi']\n",
    "\n",
    "            # Compute embeddings for the sentences\n",
    "            embedding = model.encode(hindi_sentence, convert_to_tensor=True)\n",
    "            embeddings_nllb = model.encode(translation_nllb, convert_to_tensor=True)\n",
    "            embeddings_indictrans = model.encode(translation_indictrans, convert_to_tensor=True)\n",
    "\n",
    "            # Calculate cosine similarities\n",
    "            cosine_score_nllb = util.cos_sim(embedding, embeddings_nllb).item()\n",
    "            cosine_score_indictrans = util.cos_sim(embedding, embeddings_indictrans).item()\n",
    "\n",
    "            # Select the translation with the higher cosine similarity score\n",
    "            selected_translation = translation_nllb if cosine_score_nllb > cosine_score_indictrans else translation_indictrans\n",
    "\n",
    "            # Append the selected translation\n",
    "            selected_translations.append(selected_translation)\n",
    "\n",
    "# Create a CSV file with the Hindi sentences and selected translations\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['odia_sentence', 'selected_translation'])\n",
    "\n",
    "    for i in range(len(hindi_sentences)):\n",
    "        writer.writerow([hindi_sentences[i], selected_translations[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6599c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv to tsv\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "input_csv_path = \"Task1_odi_hi_higher.csv\"  \n",
    "output_tsv_path = \"/home/f20212416/Task1_BITSP/Task1_odi_hi_2000.tsv\"  \n",
    "\n",
    "# Read data from the CSV file and write it to the TSV file with tab separators\n",
    "with open(input_csv_path, 'r', newline='', encoding='utf-8') as csv_file, open(output_tsv_path, 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "\n",
    "    for row in csv_reader:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf16b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
