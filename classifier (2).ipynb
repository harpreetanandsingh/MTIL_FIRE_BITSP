{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a3783b-8f3c-4934-a869-da9d7beb21d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.1\n",
      "  Using cached transformers-3.0.1-py3-none-any.whl (757 kB)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from transformers==3.0.1) (3.3.1)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting tokenizers==0.8.0-rc4\n",
      "  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./anaconda3/lib/python3.9/site-packages (from transformers==3.0.1) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers==3.0.1) (2021.8.3)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from transformers==3.0.1) (2.26.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.9/site-packages (from transformers==3.0.1) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.9/site-packages (from transformers==3.0.1) (4.62.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging->transformers==3.0.1) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==3.0.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==3.0.1) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==3.0.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==3.0.1) (3.2)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.9/site-packages (from sacremoses->transformers==3.0.1) (1.16.0)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.9/site-packages (from sacremoses->transformers==3.0.1) (1.1.0)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.9/site-packages (from sacremoses->transformers==3.0.1) (8.0.3)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (PEP 517) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/f20212416/anaconda3/bin/python /home/f20212416/anaconda3/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpz9ccya6f\n",
      "       cwd: /tmp/pip-install-imnmi3go/tokenizers_8e4c4216a191438596152f24f32eebfd\n",
      "  Complete output (48 lines):\n",
      "  /tmp/pip-build-env-c7z3_d7q/overlay/lib/python3.9/site-packages/setuptools/dist.py:509: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n",
      "    self.metadata.version = self._normalize_version(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-cpython-39\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers\n",
      "  copying tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/models\n",
      "  copying tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/models\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/decoders\n",
      "  copying tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/decoders\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/normalizers\n",
      "  copying tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/normalizers\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/pre_tokenizers\n",
      "  copying tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/pre_tokenizers\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/processors\n",
      "  copying tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/processors\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/trainers\n",
      "  copying tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/trainers\n",
      "  creating build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-39/tokenizers/implementations\n",
      "  copying tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers\n",
      "  copying tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/models\n",
      "  copying tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/decoders\n",
      "  copying tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/normalizers\n",
      "  copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/pre_tokenizers\n",
      "  copying tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/processors\n",
      "  copying tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-39/tokenizers/trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\n",
      "\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers==3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770b8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[1m\n",
      "Welcome to Rust!\n",
      "\u001b[m\n",
      "This will download and install the official compiler for the Rust\n",
      "programming language, and its package manager, Cargo.\n",
      "\n",
      "Rustup metadata and toolchains will be installed into the Rustup\n",
      "home directory, located at:\n",
      "\n",
      "  /home/f20212416/.rustup\n",
      "\n",
      "This can be modified with the RUSTUP_HOME environment variable.\n",
      "\n",
      "The Cargo home directory is located at:\n",
      "\n",
      "  /home/f20212416/.cargo\n",
      "\n",
      "This can be modified with the CARGO_HOME environment variable.\n",
      "\n",
      "The \u001b[1mcargo\u001b[m, \u001b[1mrustc\u001b[m, \u001b[1mrustup\u001b[m and other commands will be added to\n",
      "Cargo's bin directory, located at:\n",
      "\n",
      "  /home/f20212416/.cargo/bin\n",
      "\n",
      "This path will then be added to your \u001b[1mPATH\u001b[m environment variable by\n",
      "modifying the profile files located at:\n",
      "\n",
      "  /home/f20212416/.profile\n",
      "  /home/f20212416/.bashrc\n",
      "\n",
      "You can uninstall at any time with \u001b[1mrustup self uninstall\u001b[m and\n",
      "these changes will be reverted.\n",
      "\n",
      "Current installation options:\n",
      "\n",
      "\n",
      "\u001b[1m   \u001b[mdefault host triple: \u001b[1mx86_64-unknown-linux-gnu\u001b[m\n",
      "\u001b[1m     \u001b[mdefault toolchain: \u001b[1mstable (default)\u001b[m\n",
      "\u001b[1m               \u001b[mprofile: \u001b[1mdefault\u001b[m\n",
      "  modify PATH variable: \u001b[1myes\u001b[m\n",
      "\n",
      "1) Proceed with installation (default)\n",
      "2) Customize installation\n",
      "3) Cancel installation\n",
      ">^C\n"
     ]
    }
   ],
   "source": [
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8b165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 15 20:31:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:2B:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    34W / 250W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2880      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfefc63c-af9f-4e0c-84b7-bca212eeeb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-fbuo7fyo\n",
      "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-fbuo7fyo\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit e97deca9a3f4ddf2a6a44405ed928067d7b729f3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (4.62.3)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 382 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 29.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (1.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers==4.32.0.dev0) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (3.10.0.2)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (2021.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.32.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==4.32.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==4.32.0.dev0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==4.32.0.dev0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers==4.32.0.dev0) (2.0.4)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7459102 sha256=5d5e68f5630f94ee0c06e5424a9065c484c710801371af051f5a6684210a3d49\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_gt079pp/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.32.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d242613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 619.9 MB 27 kB/s  eta 0:00:013    |███████▉                        | 152.4 MB 1.3 MB/s eta 0:06:11     |████████████████████████▏       | 467.3 MB 2.0 MB/s eta 0:01:17\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-2.0.2-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 168.4 MB 166 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.9/site-packages (from torch) (1.9)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 64 kB/s  eta 0:00:01   |▌                               | 5.2 MB 2.3 MB/s eta 0:02:14     |██████████████▌                 | 144.1 MB 3.3 MB/s eta 0:00:53\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 54.6 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 177.1 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 102.6 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 173.2 MB 65 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 21 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 63.3 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.0.4)\n",
      "Requirement already satisfied: wheel in ./anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.9/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93583 sha256=a3254850c1740f3843b7042cb766a7d778349a79dbdc27b3302481443f26406d\n",
      "  Stored in directory: /home/f20212416/.cache/pip/wheels/a5/36/d6/cac2e6fb891889b33a548f2fddb8b4b7726399aaa2ed32b188\n",
      "Successfully built lit\n",
      "Installing collected packages: nvidia-cublas-cu11, lit, cmake, triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, torch, torchvision, torchaudio\n",
      "Successfully installed cmake-3.27.2 lit-16.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchaudio-2.0.2 torchvision-0.15.2 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8d82d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/f20212416/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - pytorch-cuda=11.7\n",
      "    - torchaudio\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.12.0               |   py39h06a4308_0        14.5 MB\n",
      "    cuda-cudart-11.7.99        |                0         194 KB  nvidia\n",
      "    cuda-cupti-11.7.101        |                0        22.9 MB  nvidia\n",
      "    cuda-libraries-11.7.1      |                0           1 KB  nvidia\n",
      "    cuda-nvrtc-11.7.99         |                0        17.3 MB  nvidia\n",
      "    cuda-nvtx-11.7.91          |                0          57 KB  nvidia\n",
      "    cuda-runtime-11.7.1        |                0           1 KB  nvidia\n",
      "    libcublas-11.10.3.66       |                0       286.1 MB  nvidia\n",
      "    libcufft-10.7.2.124        |       h4fbf590_0        93.6 MB  nvidia\n",
      "    libcusolver-11.4.0.1       |                0        78.7 MB  nvidia\n",
      "    libcusparse-11.7.4.91      |                0       151.1 MB  nvidia\n",
      "    libidn2-2.3.2              |       h7f8727e_0          81 KB\n",
      "    libnpp-11.7.4.75           |                0       129.3 MB  nvidia\n",
      "    libnvjpeg-11.8.0.2         |                0         2.2 MB  nvidia\n",
      "    libtasn1-4.16.0            |       h27cfd23_0          58 KB\n",
      "    pytorch-2.0.1              |py3.9_cuda11.7_cudnn8.5.0_0        1.19 GB  pytorch\n",
      "    pytorch-cuda-11.7          |       h778d358_5           3 KB  pytorch\n",
      "    torchaudio-2.0.2           |       py39_cu117         7.5 MB  pytorch\n",
      "    torchtriton-2.0.0          |             py39        62.5 MB  pytorch\n",
      "    torchvision-0.15.2         |       py39_cu117         7.8 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        2.05 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cuda-cudart        nvidia/linux-64::cuda-cudart-11.7.99-0\n",
      "  cuda-cupti         nvidia/linux-64::cuda-cupti-11.7.101-0\n",
      "  cuda-libraries     nvidia/linux-64::cuda-libraries-11.7.1-0\n",
      "  cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-11.7.99-0\n",
      "  cuda-nvtx          nvidia/linux-64::cuda-nvtx-11.7.91-0\n",
      "  cuda-runtime       nvidia/linux-64::cuda-runtime-11.7.1-0\n",
      "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0\n",
      "  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0\n",
      "  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0\n",
      "  libcublas          nvidia/linux-64::libcublas-11.10.3.66-0\n",
      "  libcufft           nvidia/linux-64::libcufft-10.7.2.124-h4fbf590_0\n",
      "  libcufile          nvidia/linux-64::libcufile-1.7.1.12-0\n",
      "  libcurand          nvidia/linux-64::libcurand-10.3.3.129-0\n",
      "  libcusolver        nvidia/linux-64::libcusolver-11.4.0.1-0\n",
      "  libcusparse        nvidia/linux-64::libcusparse-11.7.4.91-0\n",
      "  libiconv           pkgs/main/linux-64::libiconv-1.16-h7f8727e_2\n",
      "  libidn2            pkgs/main/linux-64::libidn2-2.3.2-h7f8727e_0\n",
      "  libnpp             nvidia/linux-64::libnpp-11.7.4.75-0\n",
      "  libnvjpeg          nvidia/linux-64::libnvjpeg-11.8.0.2-0\n",
      "  libtasn1           pkgs/main/linux-64::libtasn1-4.16.0-h27cfd23_0\n",
      "  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0\n",
      "  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1\n",
      "  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0\n",
      "  pytorch            pytorch/linux-64::pytorch-2.0.1-py3.9_cuda11.7_cudnn8.5.0_0\n",
      "  pytorch-cuda       pytorch/linux-64::pytorch-cuda-11.7-h778d358_5\n",
      "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda\n",
      "  torchaudio         pytorch/linux-64::torchaudio-2.0.2-py39_cu117\n",
      "  torchtriton        pytorch/linux-64::torchtriton-2.0.0-py39\n",
      "  torchvision        pytorch/linux-64::torchvision-0.15.2-py39_cu117\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                               4.10.3-py39h06a4308_0 --> 4.12.0-py39h06a4308_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8237225b-49d1-4c8d-bf30-4635a23b29c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 20:31:20.664773: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-15 20:31:20.703965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 20:31:21.322027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthcare\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f7fa072a0a0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f7fa072a0a0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "input_text = ['PM Modi launched a new scheme for sick patients in his address today']\n",
    "candidate_labels = ['governance-related', 'healthcare-related', 'others']\n",
    "y = classifier(input_text, candidate_labels)\n",
    "x = y[0]['labels'][0]\n",
    "z = y[0]['labels'][1]\n",
    "if(x == 'governance-related' and y[0]['scores'][0] > 0.5):\n",
    "  print('governance')\n",
    "elif(z == 'governance-related' and y[0]['scores'][1] > 0.5):\n",
    "  print('governance')\n",
    "elif(x == 'healthcare-related' and y[0]['scores'][0] > 0.5):\n",
    "  print('healthcare')  \n",
    "elif(z == 'governance-related' and y[0]['scores'][1] > 0.5):\n",
    "  print('healthcare')\n",
    "else:\n",
    "  print('others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98faefa-6c21-4658-b756-781e3604d3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "def classify_sentence(sentence):\n",
    "    if not sentence.strip():  # Check if the sentence is empty or contains only whitespace\n",
    "        return 'N/A'\n",
    "    input_text = [sentence]\n",
    "    candidate_labels = ['governance-related', 'healthcare-related', 'others']\n",
    "    a = classifier(input_text, candidate_labels)\n",
    "    x = a[0]['labels'][0]\n",
    "    z = a[0]['labels'][1]\n",
    "    y = a[0]['labels'][2]\n",
    "    if(x == 'governance-related' and a[0]['scores'][0] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(z == 'governance-related' and a[0]['scores'][1] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(y == 'governance-related' and a[0]['scores'][2] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(x == 'healthcare-related' and a[0]['scores'][0] > 0.5):\n",
    "        return 'healthcare'  \n",
    "    elif(z == 'healthcare-related' and a[0]['scores'][1] > 0.5):\n",
    "        return 'healthcare'\n",
    "    elif(z == 'healthcare-related' and a[0]['scores'][2] > 0.5):\n",
    "        return 'healthcare'\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "\n",
    "def classify_sentences_from_file(english_file, translation_file, output_file):\n",
    "    output_data = []\n",
    "    with open(english_file, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "    with open(translation_file, 'r') as trans_file:\n",
    "        translations = trans_file.readlines()\n",
    "\n",
    "    for sentence,translation in zip(sentences,translations):\n",
    "        sentence = sentence.strip()\n",
    "        translation = translation.strip()\n",
    "        if sentence and translation:\n",
    "          classified_output = classify_sentence(sentence)\n",
    "          output_data.append((sentence, translation, classified_output))\n",
    "\n",
    "    with open(output_file, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Sentence', 'Translation','Classification'])\n",
    "        writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc573eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 20:42:12.987169: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-05 20:42:13.029792: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-05 20:42:13.642159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Another implementation\n",
    "import csv\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "    if not sentence.strip():  # Check if the sentence is empty or contains only whitespace\n",
    "        return 'N/A'\n",
    "    input_text = [sentence]\n",
    "    candidate_labels = ['governance-related', 'healthcare-related', 'others']\n",
    "    a = classifier(input_text, candidate_labels)\n",
    "    x = a[0]['labels'][0]\n",
    "    z = a[0]['labels'][1]\n",
    "    y = a[0]['labels'][2]\n",
    "    if(x == 'governance-related' and a[0]['scores'][0] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(z == 'governance-related' and a[0]['scores'][1] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(y == 'governance-related' and a[0]['scores'][2] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(x == 'healthcare-related' and a[0]['scores'][0] > 0.5):\n",
    "        return 'healthcare'  \n",
    "    elif(z == 'healthcare-related' and a[0]['scores'][1] > 0.5):\n",
    "        return 'healthcare'\n",
    "    elif(z == 'healthcare-related' and a[0]['scores'][2] > 0.5):\n",
    "        return 'healthcare'\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def classify_sentences_from_file(english_file, translation_file, output_file):\n",
    "    with open(english_file, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "    with open(translation_file, 'r') as trans_file:\n",
    "        translations = trans_file.readlines()\n",
    "\n",
    "    with open(output_file, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header\n",
    "        #writer.writerow(['Sentence', 'Translation', 'Classification'])\n",
    "        \n",
    "        for sentence, translation in zip(sentences, translations):\n",
    "            sentence = sentence.strip()\n",
    "            translation = translation.strip()\n",
    "            if sentence and translation:\n",
    "                classified_output = classify_sentence(sentence)\n",
    "                writer.writerow([sentence, translation, classified_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0944151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 20:19:33.269233: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-08 20:19:34.232192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-08 20:19:37.252339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "    if not sentence.strip():  # Check if the sentence is empty or contains only whitespace\n",
    "        return 'N/A'\n",
    "    input_text = [sentence]\n",
    "    candidate_labels = ['governance-related', 'healthcare-related', 'others']\n",
    "    a = classifier(input_text, candidate_labels)\n",
    "    x = a[0]['labels'][0]\n",
    "    z = a[0]['labels'][1]\n",
    "    y = a[0]['labels'][2]\n",
    "    if(x == 'governance-related' and a[0]['scores'][0] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(z == 'governance-related' and a[0]['scores'][1] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(y == 'governance-related' and a[0]['scores'][2] > 0.5):\n",
    "        return 'governance'\n",
    "    elif(x == 'healthcare-related' and a[0]['scores'][0] > 0.5):\n",
    "        return 'healthcare'  \n",
    "    elif(z == 'healthcare-related' and a[0]['scores'][1] > 0.5):\n",
    "        return 'healthcare'\n",
    "    elif(z == 'healthcare-related' and a[0]['scores'][2] > 0.5):\n",
    "        return 'healthcare'\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "# def classify_sentence(sentence):\n",
    "#     if not sentence.strip():  # Check if the sentence is empty or contains only whitespace\n",
    "#         return 'N/A'\n",
    "    \n",
    "#     candidate_labels = ['governance-related', 'healthcare-related', 'others']\n",
    "#     a = classifier(sentence, candidate_labels)\n",
    "#     x = a[0]['labels'][0]\n",
    "#     z = a[0]['labels'][1]\n",
    "#     y = a[0]['labels'][2]\n",
    "    \n",
    "#     if (x == 'governance-related' and a[0]['scores'][0] > 0.5) or \\\n",
    "#        (z == 'governance-related' and a[0]['scores'][1] > 0.5) or \\\n",
    "#        (y == 'governance-related' and a[0]['scores'][2] > 0.5):\n",
    "#         return 'governance'\n",
    "#     elif (x == 'healthcare-related' and a[0]['scores'][0] > 0.5) or \\\n",
    "#          (z == 'healthcare-related' and a[0]['scores'][1] > 0.5) or \\\n",
    "#          (z == 'healthcare-related' and a[0]['scores'][2] > 0.5):\n",
    "#         return 'healthcare'\n",
    "#     else:\n",
    "#         return 'others'\n",
    "\n",
    "def classify_and_translate_sentences(input_file, output_file):\n",
    "    with open(input_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        sentences = [{'odia': row['odia'], 'english': row['english']} for row in reader]\n",
    "        \n",
    "    with open(output_file, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        #writer.writerow(['Hindi', 'English', 'Classification'])  # Write header\n",
    "        \n",
    "        for sentence_pair in sentences[715986:]:\n",
    "            english_sentence = sentence_pair['english']\n",
    "            odia_sentence = sentence_pair['odia']\n",
    "            if english_sentence:\n",
    "                classified_output = classify_sentence(english_sentence)\n",
    "                writer.writerow([odia_sentence, english_sentence, classified_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb568f-7916-43c6-901b-d9a319bdb627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d9a0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d9a0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d100> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d100> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d790> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d790> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072da00> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072da00> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0721a30> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0721a30> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0721970> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0721970> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f482d50b2b0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f482d50b2b0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f48149122b0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f48149122b0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d06e7550> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d06e7550> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0659940> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0659940> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a7fcf40> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a7fcf40> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0737370> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0737370> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d06e5550> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d06e5550> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0721a60> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d0721a60> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a810d30> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a810d30> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a69ec10> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a69ec10> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a69efd0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a69efd0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d06e5880> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d06e5880> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a810fd0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a810fd0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aad30> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aad30> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aaca0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aaca0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d640> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d640> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aaf70> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aaf70> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d610> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f49d072d610> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aaee0> was reported to be 1 (when accessing len(dataloader)), but 2 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/f20212416/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f467a6aaee0> was reported to be 1 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# english_file_path = 'train.eng_Latn'\n",
    "# translation_file_path = 'train.hin_Deva'\n",
    "input_file = 'odia_combined.csv'\n",
    "output_file_path = 'classification_odia_full.csv'\n",
    "\n",
    "classify_and_translate_sentences(input_file, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fae51e-960e-4cfd-be28-7d3870cd47a4",
   "metadata": {},
   "source": [
    "Counting the no. of sentences classified as governance and healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5612cd7-55ee-4642-a344-247a79dff2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 715987\n",
      "Healthcare sentences: 35506\n",
      "Governance sentences: 112457\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def count_classifications(csv_file):\n",
    "    health_count = 0\n",
    "    gov_count = 0\n",
    "    total_count = 0\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            total_count += 1\n",
    "            if row['Classification'] == 'healthcare':\n",
    "                health_count += 1\n",
    "            elif row['Classification'] == 'governance':\n",
    "                gov_count += 1\n",
    "\n",
    "    return health_count, gov_count, total_count\n",
    "def count_total(csv_file):\n",
    "    total_count = 0\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            total_count += 1\n",
    "\n",
    "    return total_count\n",
    "\n",
    "# count = count_total(\"odia_combined.csv\")\n",
    "# print(count)\n",
    "# Provide the path for the generated CSV file\n",
    "\n",
    "csv_file_path = 'classification_odia_full.csv'\n",
    "\n",
    "healthcare_count, governance_count, total_count = count_classifications(csv_file_path)\n",
    "print(f\"Total sentences: {total_count}\")\n",
    "print(f\"Healthcare sentences: {healthcare_count}\")\n",
    "print(f\"Governance sentences: {governance_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd49b71-d548-4c86-92d4-b88d20de939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"train.hin_Deva\"\n",
    "output_file_path = \"train.hin_Deva_1\"\n",
    "\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Write lines starting from line 57785 to the new file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.writelines(lines[57784:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a633573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 26 17:21:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:2B:00.0 Off |                    0 |\n",
      "| N/A   63C    P0   127W / 250W |  25285MiB / 40960MiB |     89%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2880      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   1606745      C   python                           5396MiB |\n",
      "|    0   N/A  N/A   1901250      C   ...2416/anaconda3/bin/python    14484MiB |\n",
      "|    0   N/A  N/A   1935740      C   python                           5396MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e96cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430144 rows removed from hindi_combined.csv. Result saved to hindi_combined_2.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = \"hindi_combined.csv\"\n",
    "output_file_path = \"hindi_combined_2.csv\"\n",
    "rows_to_remove = 430144\n",
    "\n",
    "with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    header = next(reader)  # Read and store the header\n",
    "    \n",
    "    # Skip rows_to_remove number of rows\n",
    "    for _ in range(rows_to_remove):\n",
    "        next(reader)\n",
    "    \n",
    "    # Write the remaining data to the output file\n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        \n",
    "        # Write the header to the output file\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Write the remaining rows to the output file\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"{rows_to_remove} rows removed from {input_file_path}. Result saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416578b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
